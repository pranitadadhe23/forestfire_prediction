{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5490c052-526b-42c1-82a1-538914a181a2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h1 style=\"text-align: center;\">Forest Fire Detection using Computer Vision</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6014341-1391-4c5f-b31e-7233b71793ee",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "Imagine you are in charge of watching a big forest. You want to keep everyone safe, but sometimes wildfires can start without warning. These fires can spread quickly and cause a lot of damage. So, we want to build a computer program that can look at pictures of forests and tell us if there’s a fire starting or not. This program can help people react faster to stop wildfires!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8dbcf-f13b-48cd-934a-927859036d0b",
   "metadata": {},
   "source": [
    "## 2. Objectives\n",
    "\n",
    "This project will:\n",
    "\n",
    "    Teach the computer to recognize pictures with wildfires and without wildfires.\n",
    "    Build a model (a smart program) that looks at forest pictures and decides if there’s a fire.\n",
    "    Show us a simple screen (a GUI) where we can upload forest pictures and get quick answers on fire danger.\n",
    "    Help keep forests and people safe by giving early warnings about fires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392c32f-06e5-4971-bb4e-491716dc63a4",
   "metadata": {},
   "source": [
    "### 3. Dataset Information\n",
    "\n",
    "To teach the computer about wildfires, we’ll use a special set of pictures called a dataset. This dataset has pictures of forests divided into two types: with a fire (wildfire) and without a fire (no wildfire). These images help the computer learn to tell the difference.\n",
    "\n",
    "    Where to Get the Dataset: You can download the dataset from Kaggle \n",
    "    https://www.kaggle.com/datasets/abdelghaniaaba/wildfire-prediction-dataset\n",
    "    Dataset Folders:\n",
    "        Inside, there are three main folders: train, test, and valid.\n",
    "        Each folder has two subfolders:\n",
    "            Wildfire: Contains pictures showing wildfires in forests.\n",
    "            NoWildfire: Contains pictures showing forests without any fire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4bb9e-3929-4f77-921d-7feef9407195",
   "metadata": {},
   "source": [
    "## 4. Understanding the CNN Model\n",
    "\n",
    "CNN stands for Convolutional Neural Network. It’s a special type of program that can look at pictures and learn to find important details. Imagine how you look at a picture and notice trees, animals, or fire — CNN does something similar!\n",
    "\n",
    "Here’s how it works, step-by-step:\n",
    "\n",
    "    Looking at Patterns: The CNN looks at lots of tiny patterns in pictures, like shapes and colors.\n",
    "    Learning from Examples: It looks at many images of wildfires and no wildfires to learn what makes them different.\n",
    "    Making Predictions: Once trained, the CNN can look at a new picture and guess if it shows a fire or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d2695-04fb-4561-80ea-8bbaa1fe0bf1",
   "metadata": {},
   "source": [
    "## 5. Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e11c51-4950-4d94-9a26-248887045b09",
   "metadata": {},
   "source": [
    "### Step 1: Importing Libraries\n",
    "\n",
    "We’ll use Python with TensorFlow/Keras for the CNN model and Tkinter for the GUI. Make sure you have these libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e45d1-2139-4bc5-8e56-e899699e353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448a68d-e3df-4625-8013-64a2350dd358",
   "metadata": {},
   "source": [
    "### Step 2: Loading and Preprocessing the Data\n",
    "\n",
    "We'll use ImageDataGenerator to load images from your dataset's train, valid, and test folders. This will resize and normalize images so our model can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25ea3572-13cd-4513-96d4-006582bc67a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30250 images belonging to 2 classes.\n",
      "Found 6300 images belonging to 2 classes.\n",
      "Found 6300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set up directories\n",
    "train_dir = r\"C:\\Users\\ASUS\\firedetection app\\train\"\n",
    "valid_dir = r\"C:\\Users\\ASUS\\firedetection app\\valid\"\n",
    "test_dir = r\"C:\\Users\\ASUS\\firedetection app\\test\"\n",
    "\n",
    "# Set up ImageDataGenerators for loading images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eee4d3-aa98-42e5-9119-9ef23ae686ae",
   "metadata": {},
   "source": [
    "### Step 3: Building the CNN Model\n",
    "\n",
    "The CNN model will have multiple convolutional layers to extract features, followed by dense layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d2fe06b-8ea3-4b2c-a43c-8b8a4b68fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Building a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification: wildfire or no wildfire\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f7023-7f86-4a62-b052-8195412c32e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b73f5c8d-b9ca-4c48-bd49-92ea41bf8f85",
   "metadata": {},
   "source": [
    "We noticed some images in our dataset are incomplete or corrupted, causing TensorFlow to fail when trying to load them. \n",
    "Here are a few steps to handle this issue:\n",
    "\n",
    "Solution: Skip Corrupted Images\n",
    "\n",
    "One way to handle this is to modify the data loading process to skip over corrupted images.\n",
    "\n",
    "    Re-import the PIL library with a setting that will ignore truncated images.\n",
    "\n",
    "    We add this line at the beginning of your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7e6cca7-cdc3-4b63-bcc2-bd33a43518e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbdde8-f817-4bf4-8a45-094606ac652c",
   "metadata": {},
   "source": [
    "### Step 4: Training the Model\n",
    "\n",
    "Train the model with the train and valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d7e0576-5d59-4511-af84-35d9d36c443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m779s\u001b[0m 822ms/step - accuracy: 0.8601 - loss: 0.3102 - val_accuracy: 0.9379 - val_loss: 0.1640\n",
      "Epoch 2/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 258ms/step - accuracy: 0.9304 - loss: 0.1876 - val_accuracy: 0.9481 - val_loss: 0.1395\n",
      "Epoch 3/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 223ms/step - accuracy: 0.9413 - loss: 0.1616 - val_accuracy: 0.9514 - val_loss: 0.1324\n",
      "Epoch 4/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 546ms/step - accuracy: 0.9456 - loss: 0.1474 - val_accuracy: 0.9565 - val_loss: 0.1261\n",
      "Epoch 5/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m796s\u001b[0m 841ms/step - accuracy: 0.9492 - loss: 0.1363 - val_accuracy: 0.9548 - val_loss: 0.1239\n",
      "Epoch 6/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 634ms/step - accuracy: 0.9567 - loss: 0.1238 - val_accuracy: 0.9400 - val_loss: 0.1735\n",
      "Epoch 7/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 767ms/step - accuracy: 0.9591 - loss: 0.1122 - val_accuracy: 0.9581 - val_loss: 0.1158\n",
      "Epoch 8/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 818ms/step - accuracy: 0.9626 - loss: 0.1004 - val_accuracy: 0.9506 - val_loss: 0.1336\n",
      "Epoch 9/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 773ms/step - accuracy: 0.9678 - loss: 0.0906 - val_accuracy: 0.9570 - val_loss: 0.1199\n",
      "Epoch 10/10\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 707ms/step - accuracy: 0.9699 - loss: 0.0829 - val_accuracy: 0.9549 - val_loss: 0.1407\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_generator, validation_data=valid_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51002bed-b6ea-4edf-a9f9-28e49f017564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"fire_detection_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4c360-bf49-4290-a484-19a030c5a818",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "With this project, we have created a simple computer program that helps spot wildfires early by looking at pictures of forests. This project shows how computer vision, a field that teaches computers to see and understand images, can help us make safer decisions for people and nature. Using CNN, we taught the computer to recognize fires, giving us a way to respond faster and prevent fires from spreading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fcbaf4-8a71-4599-9ac2-ad45668e50c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
